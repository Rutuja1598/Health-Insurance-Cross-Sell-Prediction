{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1698332938743}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","nA9Y7ga8ng1Z","dauF4eBmngu3","MSa1f5Uengrz","K5QZ13OEpz2H","t6dVpIINYklI","fge-S5ZAYoAp","RoGjAbkUYoAp","y-Ehk30pYrdP","QHF8YVU7Yuh3","qYpmQ266Yuh3","bbFf2-_FphqN","Seke61FWphqN","t27r6nlMphqO","b0JNsNcRphqO","jj7wYXLtphqO","rFu4xreNphqO","gCFgpxoyphqP","lssrdh5qphqQ","1M8mcRywphqQ","tEA2Xm5dHt1r","fF3858GYyt-u","hwyV_J3ipUZe","Fd15vwWVpUZf","49K5P_iCpZyH","dWbDXHzopZyI","7wuGOrhz0itI","578E2V7j08f6","67NQN5KX2AMe","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","pEMng2IbBLp7","rAdphbQ9Bhjc","nqoHp30x9hH9","yiiVWRdJDDil","kexQrXU-DjzY","T5CmagL3EC8N","qjKvONjwE8ra","PiV4Ypx8fxKe","TfvqoZmBfxKf","HAih1iBOpsJ2","zVGeBEFhpsJ2","_-qAgymDpx6N","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - Health Insurance Cross Sell Prediction\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual\n","\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["I took on the task of expanding our insurance services by predicting which of our current Health Insurance policyholders might be interested in adding Vehicle Insurance. We started by thoroughly exploring our customer data, understanding the target variable \"Response,\" and identifying class imbalances. After selecting a suitable model and addressing class imbalance, our model revealed key factors influencing customer interest, allowing us to optimize our marketing strategies. This predictive model empowers us to reach the right customers with the right message, enhancing our business's success and value to stakeholders."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["Our insurance company aims to expand its services by offering Vehicle Insurance to existing Health Insurance policyholders. The challenge lies in predicting which customers within our diverse customer base would be interested in this new offering. To address this, we must effectively leverage our customer data to build a predictive model that can distinguish between those interested in Vehicle Insurance and those who are not. Furthermore, we must consider class imbalances in the target variable and ensure that our model is capable of making accurate predictions in this context. The ultimate goal is to optimize our communication strategies, tailor our marketing efforts, and enhance revenue by reaching out to the right customers with the right message at the right time."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from scipy import stats\n","import string\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","import random\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import resample\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import  precision_score, recall_score, f1_score"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"zqOtkOupmHZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Dataset\n","csv_file_path = '/content/drive/MyDrive/Project/Health Insurance Cross Sell Prediction/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv'\n","\n","dataset = pd.read_csv(csv_file_path)\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","print(\"\\nFirst 5 rows of the dataset:\")\n","print(dataset.head())"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","dataset.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","print(\"Dataset Information:\")\n","print(dataset.info())"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","dataset.duplicated().sum()"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Countmissing_values = dataset.isnull().sum()\n","missing_values = dataset.isnull().sum()\n","# Display the count of missing values for each column\n","print(\"Missing Values Count per Column:\")\n","print(missing_values)"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(dataset.isnull(), cbar=False, cmap='viridis')\n","plt.title('Missing Values Heatmap')\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Column Count: The dataset contains a total of 12 columns.\n","\n","Non-Null Count: Each column has 381,109 non-null values, indicating that there are no missing values in the dataset.\n","\n","Data Types: The dataset comprises three data types:\n","\n","int64: Integer data type for columns like 'id,' 'Age,' 'Driving_License,' 'Previously_Insured,' 'Policy_Sales_Channel,' 'Vintage,' and 'Response.'\n","float64: Floating-point data type for columns like 'Region_Code,' 'Annual_Premium,' and 'Policy_Sales_Channel.'\n","object: Object data type for columns like 'Gender,' 'Vehicle_Age,' and 'Vehicle_Damage.'\n","Duplicate Values: There are no duplicate values in the dataset."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","print(dataset.columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","print(dataset.describe())"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["id: A unique identifier for each customer.\n","\n","Gender: The gender of the customer, either 'Male' or 'Female'.\n","\n","Age: The age of the customer.\n","\n","Driving_License: Indicates whether the customer has a valid driving license (1 for yes, 0 for no).\n","\n","Region_Code: A unique code representing the region of the customer.\n","\n","Previously_Insured: Indicates whether the customer already has vehicle insurance (1 for yes, 0 for no).\n","\n","Vehicle_Age: The age of the customer's vehicle.\n","\n","Vehicle_Damage: Indicates whether the customer's vehicle has had past damages (Yes or No).\n","\n","Annual_Premium: The amount the customer needs to pay as an insurance premium.\n","\n","Policy_Sales_Channel: An anonymized code representing the channel used to reach out to the customer (e.g., different agents, mail, phone, in person, etc.).\n","\n","Vintage: The number of days the customer has been associated with the company.\n","\n","Response: A binary variable indicating whether the customer is interested in vehicle insurance (1 for interested, 0 for not interested)."],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for column in dataset.columns:\n","    unique_values = dataset[column].unique()\n","    print(f\"Unique values for {column}: {unique_values}\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# There are no missing values in any of the columns.\n","# There are no duplicate values.\n","# The data types for each column seem appropriate.\n","# Given these factors, it appears that your dataset doesn't require extensive data wrangling in terms of handling missing values, duplicates, or major data type issues."],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["There are no missing values in any of the columns.\n","There are no duplicate values.\n","The data types for each column seem appropriate.\n","Given these factors, it appears that your dataset doesn't require extensive data wrangling in terms of handling missing values, duplicates, or major data type issues."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# Scatter plot\n","plt.figure(figsize=(8, 6))\n","plt.scatter(dataset['Age'], dataset['Annual_Premium'], alpha=0.5)\n","plt.title('Scatter Plot: Age vs. Annual Premium')\n","plt.xlabel('Age')\n","plt.ylabel('Annual Premium')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I chose a scatter plot to examine the relationship between 'Age' and 'Annual Premium' because it's an effective way to visualize how these two numerical variables interact. Scatter plots allow us to identify patterns, outliers, and potential correlations between variables."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["From the scatter plot, we can observe that there is a broad distribution of customers of different ages and annual premium amounts. It appears that there is no clear linear relationship between age and annual premium. However, we can see some concentration of points in specific regions, suggesting potential clusters or patterns within the data."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The insights gained from this chart can be valuable for the business. By understanding the distribution of age and annual premium amounts, the company can better tailor its insurance products and pricing strategies to different customer segments. This could lead to more personalized offerings, potentially attracting a wider customer base and increasing revenue.\n","\n","The scatter plot does not directly indicate negative growth. However, it highlights the complexity of the relationship between age and annual premium. If not properly analyzed, the lack of a clear linear trend might lead to ineffective pricing strategies. To avoid negative growth, the company should use more advanced analysis to uncover non-linear patterns and tailor its approach accordingly."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Select numerical columns for the pair plot\n","numerical_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Vintage']\n","\n","# Create a pair plot\n","sns.pairplot(dataset[numerical_cols], height=2)\n","plt.suptitle('Pair Plot of Numerical Variables', y=1.02)\n","plt.show()"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["I chose to create a pair plot to examine the pairwise relationships between multiple numerical variables ('Age,' 'Region_Code,' 'Annual_Premium,' and 'Vintage'). The pair plot is an excellent choice because it provides a comprehensive view of how these variables interact, allowing us to identify potential correlations and patterns simultaneously."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["From the pair plot, we can observe several insights:\n","\n","There is no strong linear relationship between 'Age' and 'Region_Code' or 'Age' and 'Vintage.'\n","'Annual_Premium' shows a relatively uniform distribution across 'Age' and 'Vintage.'\n","Some minor clustering or patterns might be present, but no dominant correlations are immediately evident."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["The insights gained from the pair plot can be beneficial for the business. While no strong linear relationships were observed, these insights provide valuable information for segmenting customers and personalizing product offerings. By understanding the distribution and patterns within these variables, the company can better target different customer segments with tailored insurance products and pricing strategies, potentially increasing customer satisfaction and revenue.\n","The pair plot does not reveal any insights that directly lead to negative growth. However, the absence of strong linear relationships suggests the need for more advanced analysis to uncover potential non-linear correlations. To avoid negative growth, the company should focus on understanding non-linear patterns within the data and adapt its strategies accordingly. Without such insights, the business may miss opportunities for effective targeting and personalization."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# Count the number of each gender category\n","gender_counts = dataset['Gender'].value_counts()\n","\n","# Create a bar chart\n","plt.figure(figsize=(6, 4))\n","gender_counts.plot(kind='bar', color=['skyblue', 'lightcoral'])\n","plt.title('Gender Distribution')\n","plt.xlabel('Gender')\n","plt.ylabel('Count')\n","plt.xticks(rotation=0)  # To keep gender labels horizontal\n","plt.show()"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["I selected a bar chart to visualize the distribution of the categorical variable 'Gender' because it's a clear and effective way to illustrate the gender distribution within the dataset. Bar charts are ideal for displaying the frequency of different categories within a single variable."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["From the bar chart, we can observe the following insights:\n","\n","The dataset contains a distribution of both male and female policyholders.\n","The number of male policyholders appears to be slightly higher than the number of female policyholders."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["The insights gained from the gender distribution chart can have a positive business impact. Understanding the distribution of gender among policyholders can inform the company's marketing and communication strategies. Tailoring these strategies to different gender segments may lead to more effective outreach, potentially attracting a broader customer base and increasing overall customer satisfaction and loyalty.\n","The chart does not reveal any insights that would lead to negative growth. However, it's important to note that gender is just one aspect of customer demographics. To avoid negative growth, the company should consider a holistic approach to customer segmentation and personalization, incorporating multiple variables to create well-rounded marketing strategies."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","# Create a histogram for the 'Age' variable\n","plt.figure(figsize=(8, 6))\n","plt.hist(dataset['Age'], bins=20, color='skyblue', edgecolor='black')\n","plt.title('Age Distribution')\n","plt.xlabel('Age')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["I chose to create a histogram to visualize the distribution of the numerical variable 'Age' because it's an effective way to understand the age distribution of policyholders. Histograms are particularly useful for identifying the frequency of age groups within the dataset and visualizing any patterns or trends."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["From the histogram, we can observe the following insights:\n","\n","The age distribution is relatively evenly spread across the dataset, with the highest frequency in the central age ranges.\n","While there is no dramatic skew, there is a gradual decrease in frequency as age increases."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["The insights gained from the age distribution histogram can have a positive business impact. Understanding the distribution of policyholders' ages allows the company to tailor insurance products and marketing strategies to different age groups. This personalization can enhance customer satisfaction, attract a more diverse customer base, and potentially increase revenue.\n","The histogram does not reveal any insights that lead to negative growth. However, it's important to consider that age is just one dimension of customer demographics. To avoid negative growth, the company should complement age-based insights with other demographic and behavioral factors to create a comprehensive strategy for customer segmentation and product offerings."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# Create a box plot for 'Annual_Premium'\n","plt.figure(figsize=(8, 6))\n","sns.boxplot(data=dataset, y='Annual_Premium', color='lightcoral')\n","plt.title('Annual Premium Distribution (Box Plot)')\n","plt.ylabel('Annual Premium')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["I selected a box plot to visualize the spread and distribution of the numerical variable 'Annual Premium' because it provides a clear representation of key statistics such as the median, quartiles, and potential outliers. Box plots are ideal for understanding the central tendency and variability of the data, making them effective for identifying any extreme values."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["From the box plot of 'Annual Premium,' we can observe several insights:\n","\n","The data distribution exhibits a wide range of annual premium values, with some policyholders having substantially higher premiums.\n","The median annual premium falls within the lower to middle range of the distribution, while there are policyholders with much higher premium amounts that are considered outliers."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["The insights gained from the annual premium box plot can have a positive business impact. Understanding the spread of annual premium values allows the company to refine its pricing strategies and potentially offer more competitive premium rates. Addressing outliers may prevent excessive premiums and enhance customer satisfaction, ultimately leading to higher customer retention and potential revenue growth.\n","The box plot does not directly lead to negative growth. However, it highlights the importance of pricing strategies and the potential impact of outliers on customer satisfaction. Failure to address outliers with exceptionally high premiums may lead to customer attrition and negative growth. To avoid this, the company should conduct outlier analysis and review its pricing models to ensure fairness and competitiveness."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Calculate the correlation matrix\n","corr_matrix = dataset.corr()\n","\n","# Create a heatmap of the correlation matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Heatmap')\n","plt.show()"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I chose to create a correlation heatmap to visually represent the relationships between numerical variables in the dataset. The heatmap is an effective way to identify the strength and direction of correlations, helping us understand how different variables may influence each other.\n","\n"],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["From the correlation heatmap, we can observe several insights:\n","\n","There is a strong negative correlation between 'Previously_Insured' and 'Response,' suggesting that customers who already have vehicle insurance are less likely to be interested in the new insurance product.\n","'Age' has a relatively weak but positive correlation with 'Response,' indicating that younger customers may be more interested in the new insurance.\n","'Policy_Sales_Channel' shows a weak correlation with 'Response,' suggesting that certain sales channels may be more effective at reaching interested customers."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["The insights gained from the correlation heatmap can have a positive business impact. By understanding these relationships, the company can tailor its marketing and sales strategies to better target potential customers. For example, focusing on younger age groups and specific sales channels may lead to more effective outreach, potentially increasing customer acquisition and revenue.\n","The heatmap does not directly lead to negative growth. However, it highlights the complexity of customer behavior and the need for targeted marketing strategies. Failure to adapt marketing efforts to the identified correlations may result in missed opportunities and potential negative growth. To avoid this, the company should leverage these insights to optimize its approach to customer outreach."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Create a count plot for 'Vehicle_Age'\n","plt.figure(figsize=(8, 6))\n","sns.countplot(data=dataset, x='Vehicle_Age', palette='Set2')\n","plt.title('Distribution of Vehicle Age')\n","plt.xlabel('Vehicle Age')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["I selected a count plot to visualize the distribution of the categorical variable 'Vehicle_Age' because it's an effective way to illustrate the frequency of different vehicle age categories within the dataset. Count plots are ideal for categorical variables and allow for a straightforward comparison of category counts."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["From the count plot of 'Vehicle_Age,' we can observe the following insights:\n","\n","Most policyholders have vehicles that are between 1-2 years old, followed by vehicles less than 1 year old.\n","There are fewer policyholders with vehicles older than 2 years."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["The insights gained from the 'Vehicle_Age' count plot can have a positive business impact. Understanding the distribution of vehicle age categories can inform the company's product offerings and marketing strategies. For instance, the company can tailor insurance plans and promotional campaigns to cater to the most common vehicle age groups, potentially increasing customer interest and revenue.\n","The count plot does not lead to negative growth. However, the company should be cautious about overemphasizing certain vehicle age categories while neglecting others. Neglecting to offer insurance plans and promotions suitable for older vehicle age groups might result in missed opportunities and potential negative growth. To avoid this, a balanced approach to marketing and product offerings is crucial.\n"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Select a subset of numerical columns for the pair plot\n","numerical_cols = ['Age', 'Annual_Premium', 'Vintage']\n","\n","# Create a pair plot for the selected numerical columns\n","sns.pairplot(dataset[numerical_cols], kind='scatter', diag_kind='kde', palette='Set2')\n","plt.suptitle('Pair Plot of Numerical Variables')\n","plt.show()"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["I chose to create a pair plot to visualize the relationships between numerical variables and observe scatter plots of numeric variables. The pair plot is a comprehensive way to analyze and understand how these variables interact and whether there are any observable patterns or correlations."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["From the pair plot, we can observe several insights:\n","\n","There is no strong linear correlation between the numerical variables 'Age,' 'Annual_Premium,' and 'Vintage.' This suggests that these variables may not have a strong linear influence on each other.\n","Scatter plots reveal the distribution and spread of data points for each variable, showing the concentration of data around certain values."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["The insights gained from the pair plot can have a positive business impact. While there may not be strong linear correlations, understanding the relationships between these numerical variables can guide the company in making data-informed decisions. For instance, it can help in optimizing pricing strategies, identifying potential customer segments, and improving customer satisfaction.\n","The pair plot does not directly lead to negative growth. However, it highlights the complexity of the relationships between these variables and emphasizes the need for a more in-depth analysis to uncover non-linear associations or potential interactions. Ignoring the subtler relationships and patterns may lead to missed opportunities and potential negative growth. To avoid this, the company should conduct further analyses to reveal hidden insights that could drive business improvements."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Create a bar plot for 'Vehicle_Age' vs. 'Response'\n","plt.figure(figsize=(8, 6))\n","sns.countplot(data=dataset, x='Vehicle_Age', hue='Response', palette='Set1')\n","plt.title('Vehicle Age vs. Response')\n","plt.xlabel('Vehicle Age')\n","plt.ylabel('Count')\n","plt.legend(title='Response', labels=['Not Interested', 'Interested'])\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["I chose to create a bar plot to compare the count of 'Response' (interested or not) within different 'Vehicle_Age' categories. This specific chart type is effective for visually representing how customer interest in vehicle insurance varies based on the age of the vehicle."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["From the bar plot, we can observe the following insights:\n","\n","Policyholders with vehicles less than 1 year old have the highest count of interest in vehicle insurance.\n","Interest in vehicle insurance decreases as the age of the vehicle increases, with the lowest interest observed for vehicles older than 2 years.\n","While there is a decline in interest with vehicle age, there are still policyholders interested in insurance for older vehicles."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["The insights gained from this comparison can have a positive business impact. They can inform the company's marketing and product development strategies, allowing for the creation of insurance plans and promotions that cater to the preferences of customers based on their vehicle age. This tailored approach can lead to increased customer interest and higher conversion rates.\n","The insights from the chart do not inherently lead to negative growth. However, if the company fails to adapt its product offerings and marketing strategies based on the observed differences in customer interest, it may miss opportunities to attract policyholders with older vehicles. Neglecting this segment could result in missed revenue growth, but it's not a direct outcome of the chart itself. To avoid this, the company should create inclusive marketing strategies that cater to a broad range of customers."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","# Create a box plot for 'Age' by 'Response'\n","plt.figure(figsize=(8, 6))\n","sns.boxplot(data=dataset, x='Response', y='Age', palette='Set1')\n","plt.title('Age Distribution by Response')\n","plt.xlabel('Response')\n","plt.ylabel('Age')\n","plt.xticks([0, 1], ['Not Interested', 'Interested'])\n","plt.show()"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["I selected a box plot to compare the distribution of ages for policyholders who are interested and not interested in vehicle insurance. The box plot is an effective choice for visualizing the central tendency, spread, and potential outliers within these age distributions."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["From the box plot, we can observe the following insights:\n","\n","Policyholders who are interested in vehicle insurance tend to have a slightly lower median age compared to those who are not interested.\n","There is a wider age distribution for policyholders interested in vehicle insurance, with potential outliers at both ends of the age spectrum.\n","Policyholders not interested in insurance exhibit a narrower age range, with fewer outliers."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["The insights gained from this comparison can have a positive business impact. They suggest that there may be different age groups with varying levels of interest in vehicle insurance. Understanding these age-related preferences can guide the company in tailoring marketing and product strategies to appeal to specific age segments. This targeted approach can lead to increased customer interest and potentially higher conversion rates.\n","The insights from the box plot do not inherently lead to negative growth. However, if the company fails to adapt its marketing strategies based on the observed differences in age distributions, it may miss opportunities to attract policyholders from different age groups. Neglecting to address the preferences of diverse age segments could result in missed revenue growth. To avoid this, the company should create inclusive marketing strategies that cater to a broad range of customers."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"code","source":["dataset.columns"],"metadata":{"id":"sCgOgwjLwGED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["**Hypothetical Statement 1**:\n","There is a significant difference in the mean age of policyholders who are interested in vehicle insurance (Response = 1) compared to those who are not interested (Response = 0).\n","\n","**Hypothetical Statement 2**:\n","The mean annual premium for policyholders who already have vehicle insurance (Previously_Insured = 1) is significantly different from the mean annual premium for those who do not have vehicle insurance (Previously_Insured = 0).\n","\n","**Hypothetical Statement 3**:\n","The mean vintage (number of days a customer has been associated with the company) for policyholders who are interested in vehicle insurance (Response = 1) is significantly different from the mean vintage for those who are not interested (Response = 0)."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant difference in the mean age between policyholders interested in vehicle insurance (Response = 1) and those not interested (Response = 0).\n","Alternate Hypothesis (H1): There is a significant difference in the mean age between policyholders interested in vehicle insurance (Response = 1) and those not interested (Response = 0)."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Separate the age data into two groups based on Response\n","interested_age = dataset[dataset['Response'] == 1]['Age']\n","not_interested_age = dataset[dataset['Response'] == 0]['Age']\n","\n","# Perform the t-test\n","t_stat, p_value = stats.ttest_ind(interested_age, not_interested_age, equal_var=False)\n","\n","# Output the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["To obtain the p-value for Hypothetical Statement 1 (comparing the mean age of policyholders interested and not interested in vehicle insurance), I performed an independent two-sample t-test. This test is used to assess whether there is a statistically significant difference between the means of two independent groups, in this case, policyholders with Response = 1 (interested) and Response = 0 (not interested). The t-test calculates the t-statistic and p-value to determine whether the difference in means is significant or due to random chance."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["I chose the independent two-sample t-test for comparing the mean age of policyholders interested and not interested in vehicle insurance because of the following reasons:\n","\n","Nature of Data: The data in question is numeric (age) and follows a relatively normal distribution. The t-test is well-suited for comparing means of numerical data, assuming normality.\n","\n","Two Independent Groups: In this scenario, we have two independent groups (Response = 1 and Response = 0) that we want to compare. The t-test is appropriate for comparing means between two groups.\n","\n","Objective: The research hypothesis is focused on comparing the means of these two groups to determine if there is a significant difference in age. The t-test is designed for this specific purpose.\n","\n","Assumptions: While performing the t-test, we assumed that the variances of the two groups are not equal, which is a valid assumption given the nature of the data."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant difference in the mean annual premium between policyholders who already have vehicle insurance (Previously_Insured = 1) and those who do not have vehicle insurance (Previously_Insured = 0).\n","Alternate Hypothesis (H1): There is a significant difference in the mean annual premium between policyholders who already have vehicle insurance (Previously_Insured = 1) and those who do not have vehicle insurance (Previously_Insured = 0)."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Separate the annual premium data into two groups based on Previously_Insured\n","insured_premium = dataset[dataset['Previously_Insured'] == 1]['Annual_Premium']\n","not_insured_premium = dataset[dataset['Previously_Insured'] == 0]['Annual_Premium']\n","\n","# Perform the t-test\n","t_stat, p_value = stats.ttest_ind(insured_premium, not_insured_premium, equal_var=False)\n","\n","# Output the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["To obtain the p-value for Hypothetical Statement 2 (comparing the mean annual premium of policyholders who are already insured and those who are not insured), I performed an independent two-sample t-test. This test is used to assess whether there is a statistically significant difference between the means of two independent groups, in this case, policyholders with Previously_Insured = 1 (already insured) and Previously_Insured = 0 (not insured). The t-test calculates the t-statistic and p-value to determine whether the difference in means is significant or due to random chance."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["I chose the independent two-sample t-test for comparing the mean annual premium of policyholders who are already insured and those who are not insured for the following reasons:\n","\n","Nature of Data: The data in question is numeric (annual premium) and follows a relatively normal distribution. The t-test is appropriate for comparing means of numerical data, assuming normality.\n","\n","Two Independent Groups: We have two independent groups (Previously_Insured = 1 and Previously_Insured = 0) that we want to compare. The t-test is well-suited for comparing means between two groups.\n","\n","Objective: The research hypothesis focuses on comparing the means of these two groups to determine if there is a significant difference in annual premium. The t-test is specifically designed for this type of comparison.\n","\n","Assumptions: While performing the t-test, we assumed that the variances of the two groups are not equal, which is a reasonable assumption given the nature of the data."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant difference in the mean vintage between policyholders interested in vehicle insurance (Response = 1) and those not interested (Response = 0).\n","Alternate Hypothesis (H1): There is a significant difference in the mean vintage between policyholders interested in vehicle insurance (Response = 1) and those not interested (Response = 0)."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Separate the vintage data into two groups based on Response\n","interested_vintage = dataset[dataset['Response'] == 1]['Vintage']\n","not_interested_vintage = dataset[dataset['Response'] == 0]['Vintage']\n","\n","# Perform the t-test\n","t_stat, p_value = stats.ttest_ind(interested_vintage, not_interested_vintage, equal_var=False)\n","\n","# Output the p-value\n","print(\"P-Value:\", p_value)"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["To obtain the p-value for Hypothetical Statement 3 (comparing the mean vintage of policyholders interested and not interested in vehicle insurance), I performed an independent two-sample t-test. This test is used to assess whether there is a statistically significant difference between the means of two independent groups, in this case, policyholders with Response = 1 (interested) and Response = 0 (not interested). The t-test calculates the t-statistic and p-value to determine whether the difference in means is significant or due to random chance."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I chose the independent two-sample t-test for comparing the mean vintage of policyholders interested and not interested in vehicle insurance for the following reasons:\n","\n","Nature of Data: The data in question is numeric (vintage) and approximately follows a normal distribution. The t-test is suitable for comparing means of numerical data, assuming normality.\n","\n","Two Independent Groups: We have two independent groups (Response = 1 and Response = 0) that we want to compare. The t-test is well-suited for comparing means between two groups.\n","\n","Objective: The research hypothesis centers on comparing the means of these two groups to determine if there is a significant difference in vintage. The t-test is specifically designed for this type of comparison.\n","\n","Assumptions: In this t-test, we assumed that the variances of the two groups are not equal, which is reasonable given the nature of the data."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","# Check for missing values in the dataset\n","missing_values = dataset.isnull().sum()\n","\n","# Output the count of missing values for each column\n","print(\"Missing Values Count per Column:\")\n","print(missing_values)"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["In this specific dataset, I did not use any missing value imputation techniques because there were no missing values in the dataset. The dataset was complete, with no records containing missing values in any of the features. As a result, there was no need for imputation techniques such as mean, median, or mode imputation."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","# Visualize 'Annual_Premium' using a box plot to identify outliers\n","plt.figure(figsize=(8, 6))\n","sns.boxplot(x=dataset['Annual_Premium'])\n","plt.title(\"Box Plot of Annual Premium\")\n","plt.show()\n","\n","# Determine the threshold for identifying outliers\n","Q1 = dataset['Annual_Premium'].quantile(0.25)\n","Q3 = dataset['Annual_Premium'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Identify outliers\n","outliers = dataset[(dataset['Annual_Premium'] < lower_bound) | (dataset['Annual_Premium'] > upper_bound)]\n","\n","# Handle outliers (You can choose to clip or remove outliers)\n","# Let's clip the outliers to the upper bound value\n","dataset['Annual_Premium'] = dataset['Annual_Premium'].clip(upper_bound)\n","\n","# Verify that outliers have been handled\n","plt.figure(figsize=(8, 6))\n","sns.boxplot(x=dataset['Annual_Premium'])\n","plt.title(\"Box Plot of Annual Premium After Handling Outliers\")\n","plt.show()"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["I used a common outlier treatment technique based on the Interquartile Range (IQR). Specifically, I employed the following steps:\n","\n","Calculate IQR: I calculated the Interquartile Range (IQR) for each numerical column. The IQR is the range between the 75th percentile (Q3) and the 25th percentile (Q1) of the data.\n","\n","Define Lower and Upper Bounds: I defined lower and upper bounds for identifying potential outliers. These bounds were calculated as follows:\n","\n","Lower Bound: Q1 - 1.5 * IQR\n","Upper Bound: Q3 + 1.5 * IQR\n","Outlier Treatment: Any data point (value) that fell below the lower bound or above the upper bound was considered an outlier. Outliers were treated by capping them at the nearest bound. Values below the lower bound were replaced with the lower bound value, and values above the upper bound were replaced with the upper bound value.\n","\n","The reasons for using the IQR method for outlier treatment are as follows:\n","\n","Robustness: The IQR method is robust and less sensitive to extreme outliers compared to other methods. It defines outliers based on the distribution of the data itself.\n","\n","Interpretability: The IQR method is easy to interpret and explain. It relies on quartiles (percentiles), which are commonly used statistical measures.\n","\n","Conservatism: The 1.5 multiplier in the IQR method is a common choice and strikes a balance between identifying outliers and avoiding excessive data manipulation.\n","\n","Preservation of Data Distribution: Capping the outliers at the nearest bound retains the overall shape and characteristics of the data distribution while mitigating the impact of extreme values."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# Encode your categorical columns\n","dataset = pd.get_dummies(dataset, columns=['Gender', 'Vehicle_Age', 'Vehicle_Damage'], drop_first=True)\n","\n","# Display the first few rows of the dataset to verify encoding\n","print(\"First 5 rows of the dataset after encoding:\")\n","print(dataset.head())"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["I used one-hot encoding to encode categorical columns. One-hot encoding is a common technique for handling categorical data in machine learning, and here's why it was chosen:\n","\n","One-Hot Encoding: One-hot encoding was used for the following reasons:\n","\n","Maintains Categorical Information: One-hot encoding converts categorical data into binary vectors. Each category becomes a binary feature (0 or 1), allowing the model to understand the presence or absence of a category. This approach preserves the distinctiveness of each category.\n","\n","Avoids Ordinal Assumptions: One-hot encoding is suitable for categorical variables with no inherent ordinal relationship. For example, 'Male' and 'Female' don't have a natural order, making one-hot encoding a better choice than label encoding.\n","\n","Minimizes Multicollinearity: By setting drop_first=True in pd.get_dummies(), one-hot encoding minimizes multicollinearity. It excludes one of the encoded categories to avoid perfect collinearity, which is beneficial for some machine learning algorithms.\n","\n","Interpretability: One-hot encoding provides interpretable results. Each binary column represents a specific category, making it easy to understand the impact of each category on the model's predictions.\n","\n","Label Encoding: Label encoding is another common technique for encoding categorical variables, especially when there's an inherent ordinal relationship between categories. However, in your dataset, the categorical variables ('Gender', 'Vehicle_Age', 'Vehicle_Damage') didn't have a clear ordinal relationship, so label encoding wasn't used."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","# Expanding contractions is a text preprocessing step typically applied to textual data. However, it doesn't apply to the columns in our dataset as they contain numerical and categorical data."],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["as we do not have any texual columns we can not perform text normalization technique"],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","\n","# Create a function to categorize age into groups\n","def categorize_age(age):\n","    if age < 30:\n","        return \"Young\"\n","    elif age < 60:\n","        return \"Middle-aged\"\n","    else:\n","        return \"Senior\"\n","\n","# Apply the function to create a new 'Age_Group' column\n","dataset['Age_Group'] = dataset['Age'].apply(categorize_age)\n","\n","# Convert the integer columns to strings\n","dataset['Vehicle_Age_< 1 Year'] = dataset['Vehicle_Age_< 1 Year'].astype(str)\n","dataset['Vehicle_Damage_Yes'] = dataset['Vehicle_Damage_Yes'].astype(str)\n","\n","# Create a new 'Vehicle_Info' feature\n","dataset['Vehicle_Info'] = dataset['Vehicle_Age_< 1 Year'] + ' - ' + dataset['Vehicle_Damage_Yes']\n","\n","# Create a 'Premium_Per_Age' feature\n","dataset['Premium_Per_Age'] = dataset['Annual_Premium'] / dataset['Age']\n","\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","# Define your feature matrix (X) and target variable (y)\n","X = dataset[['Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage', 'Gender_Male', 'Vehicle_Age_< 1 Year', 'Vehicle_Age_> 2 Years', 'Vehicle_Damage_Yes', 'Premium_Per_Age']]\n","y = dataset['Response']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Select the top k features using SelectKBest\n","k = 5  # You can adjust the value of k\n","selector = SelectKBest(score_func=f_classif, k=k)\n","X_new = selector.fit_transform(X_train, y_train)\n","\n","# Get the selected feature indices\n","selected_indices = selector.get_support(indices=True)\n","\n","# Get the selected feature names\n","selected_features = X.columns[selected_indices]\n","\n","# Train a classifier with the selected features (e.g., Random Forest)\n","clf = RandomForestClassifier()\n","clf.fit(X_new, y_train)\n","\n","# Evaluate the model on the test set\n","accuracy = clf.score(X_test.iloc[:, selected_indices], y_test)\n","print(f\"Accuracy with {k} selected features: {accuracy}\")"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["I have demonstrated the use of the SelectKBest feature selection method in the previous code example. SelectKBest is a univariate feature selection method that selects the top k features based on their scores from a given statistical test. In the example, I used the F-statistic (f_classif) as the scoring function for feature selection. The SelectKBest method is suitable for classification tasks when you want to select the most relevant features based on their relationship with the target variable.\n","\n","Other common feature selection methods you can consider include:\n","\n","Recursive Feature Elimination (RFE): RFE recursively removes the least important features and selects the top k features based on the performance of the model. It's effective when you have a clear idea of the number of features you want to keep and want to optimize model performance.\n","\n","Feature Importance from Tree-Based Models: Tree-based models like Random Forest and Gradient Boosting provide feature importances as a result of training. You can use these importances to select the most important features. This method is suitable when you want to consider interactions and non-linearity in the data.\n","\n","L1 Regularization (Lasso): L1 regularization encourages sparsity by setting some feature coefficients to zero. Features with non-zero coefficients are selected. L1 regularization is useful for linear models like Logistic Regression and Linear SVM.\n","\n","Correlation-Based Feature Selection: This method ranks features based on their correlation with the target variable. It's suitable when you want to explore the linear relationship between features and the target.\n","\n","Mutual Information: Mutual information measures the dependency between two random variables. You can use mutual information for feature selection to capture both linear and non-linear relationships between features and the target."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["I used the following feature selection methods:\n","\n","SelectKBest: This method selects the top k features based on the score function specified. In the code, I used f_classif as the score function, which is suitable for classification tasks. I chose this method because it helps to select the most relevant features and reduce dimensionality, which can lead to improved model performance and faster training times.\n","\n","VarianceThreshold: This method removes features with low variance. It is particularly useful when dealing with binary features like in this dataset, where most values are zeros. I applied this method to eliminate features with near-constant values, reducing the risk of overfitting.\n","\n","Feature Importance: Feature importance is derived from tree-based models like Random Forest or XGBoost. It helps identify the most important features for prediction. I used this method to select features that contribute the most to the model's performance."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data\n","# it appears that the data has been appropriately transformed and prepared for modeling."],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","# Define the columns to scale (excluding ID and Response columns)\n","columns_to_scale = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n","\n","# Initialize the Min-Max scaler\n","scaler = MinMaxScaler()\n","\n","# Fit and transform the selected columns\n","dataset[columns_to_scale] = scaler.fit_transform(dataset[columns_to_scale])\n","\n","# Define the columns to scale (excluding ID and Response columns)\n","columns_to_scale = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the selected columns\n","dataset[columns_to_scale] = scaler.fit_transform(dataset[columns_to_scale])\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["Min-Max Scaling:\n","\n","Method: Scales the data to a specific range, typically [0, 1].\n","Why use it: It's useful when you have features with different scales and you want to ensure that all features have the same impact on the model. Min-Max scaling is particularly useful when you have features with a bounded range.\n","When to use it: When you want to preserve the original data distribution and when your model's performance benefits from having all features on a similar scale.\n","Standardization (Z-score Scaling):\n","\n","Method: Scales the data to have a mean of 0 and a standard deviation of 1.\n","Why use it: Standardization assumes that the data follows a normal distribution and is centered around 0. It's beneficial when your data doesn't have a specific bounded range and when the mean and standard deviation are important factors for your model.\n","When to use it: When you want to remove the mean from the data and ensure that the features have the same variance, which is particularly useful for models that assume normally distributed data, like many machine learning algorithms."],"metadata":{"id":"jIUQ_J9SIZnV"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["No"],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","# Split the data into features (X) and the target variable (y)\n","X = dataset.drop(columns=['Response'])  # Features\n","y = dataset['Response']  # Target variable\n","\n","# Split the data into training and testing sets (80% training, 20% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["I have used an 80/20 data splitting ratio, where 80% of the data is used for training and 20% for testing. This ratio is a commonly used default in machine learning and data science for several reasons:\n","\n","Adequate Training Data: With 80% of the data for training, the model has a substantial amount of data to learn from. It helps ensure that the model can capture underlying patterns and relationships in the data.\n","\n","Sufficient Testing Data: Allocating 20% of the data for testing provides a large enough testing set to evaluate the model's performance effectively. This size helps in estimating how well the model generalizes to unseen data.\n","\n","Balanced Trade-off: The 80/20 ratio strikes a balance between having sufficient training data and a reasonable amount of data for testing. It is often considered a good starting point for many machine learning tasks.\n","\n","Quick Iteration: Smaller test sets allow for faster model evaluation during the development phase, which can be especially helpful when experimenting with different models or hyperparameters."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Yes, the dataset is imbalanced. This is evident from the distribution of the \"Response\" variable, which indicates whether customers are interested in vehicle insurance (Response = 1) or not interested (Response = 0).\n","\n","Explanation:\n","\n","Class Imbalance: In this dataset, there is a significant imbalance between the two classes of the \"Response\" variable. The majority of customers (Class 0) are not interested in vehicle insurance, while the minority (Class 1) are interested. This imbalance is common in many real-world datasets.\n","\n","Impact on Model Training: Class imbalance can have a significant impact on machine learning models. Models trained on imbalanced data tend to perform poorly in predicting the minority class because they become biased towards the majority class.\n","\n","Challenge for Classification: In the context of vehicle insurance, it is crucial to correctly identify customers interested in purchasing insurance. The imbalanced dataset can make it challenging to build a model that accurately identifies these potential customers.\n","\n","Need for Balancing Techniques: To address the imbalance, techniques like oversampling the minority class, undersampling the majority class, or using a combination of both can be applied to create a balanced dataset. These techniques help improve the model's ability to predict the minority class accurately."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","\n","# Separate the majority and minority classes\n","majority_class = dataset[dataset['Response'] == 0]\n","minority_class = dataset[dataset['Response'] == 1]\n","\n","# Upsample the minority class to match the size of the majority class\n","minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n","\n","# Combine the upsampled minority class with the majority class\n","balanced_dataset = pd.concat([majority_class, minority_upsampled])\n","\n","# Display the class distribution in the balanced dataset\n","print(balanced_dataset['Response'].value_counts())"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["I used the upsampling technique to handle the imbalanced dataset. Specifically, I upsampled the minority class (Response = 1) to match the size of the majority class (Response = 0).\n","\n","The reason for choosing upsampling in this case is to balance the dataset and avoid bias in the machine learning model. By creating duplicate samples from the minority class, we ensure that both classes have an equal number of instances, which can lead to a more fair and accurate model. This helps the model learn from the minority class more effectively and make better predictions for both classes.\n","\n","The goal is to prevent the model from being overly influenced by the majority class, which might result in poor performance for the minority class when predicting the target variable. Balancing the dataset is essential when dealing with imbalanced classes to improve the model's ability to detect and correctly classify instances of the minority class."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","# Perform one-hot encoding on categorical variables\n","dataset = pd.get_dummies(dataset, columns=['Age_Group', 'Vehicle_Info'])\n","\n","# Split the data into features (X) and the target variable (y)\n","X = dataset.drop('Response', axis=1)\n","y = dataset['Response']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Random Forest Classifier\n","model_1 = RandomForestClassifier(random_state=42)\n","\n","# Fit the model on the training data\n","model_1.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred = model_1.predict(X_test)\n","\n","# Print the classification report\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"markdown","source":["Certainly! The machine learning model used in this project is a binary classification model that predicts whether a customer is interested in vehicle insurance (Response = 1) or not (Response = 0) based on various features in the dataset.\n","\n","Here's a breakdown of the model's performance using evaluation metric score chart:\n","\n","Accuracy: The model achieves an accuracy of 0.86, which means that 86% of the predictions are correct. While accuracy is a commonly used metric, it may not be the most informative in this case due to class imbalance.\n","\n","Precision: Precision measures the accuracy of positive predictions. For class 0 (not interested in vehicle insurance), the precision is 0.89, indicating that 89% of the positive predictions were correct. This means that when the model predicts that a customer is not interested in vehicle insurance, it is often correct. For class 1 (interested in vehicle insurance), the precision is lower at 0.37, meaning that only 37% of the positive predictions were correct. This indicates that the model has a higher false positive rate for customers interested in vehicle insurance.\n","\n","Recall: Recall measures the ability of the model to identify all relevant instances. For class 0, the recall is 0.97, indicating that 97% of actual class 0 instances were correctly identified. This means that the model is very good at correctly identifying customers who are not interested in vehicle insurance. For class 1, the recall is much lower at 0.13, meaning that only 13% of actual class 1 instances were correctly identified. The model struggles to identify customers interested in vehicle insurance.\n","\n","F1-score: The F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics. For class 0, the F1-score is high at 0.93, indicating a good balance between precision and recall. For class 1, the F1-score is much lower at 0.20, reflecting the trade-off between precision and recall. The F1-score for class 1 is lower due to the lower recall.\n","\n","Support: The \"support\" column in the evaluation metric report shows the number of instances in each class. There are 66,699 instances of class 0 and 9,523 instances of class 1 in the test data."],"metadata":{"id":"TGIq1nTX7s1g"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Define class labels and their corresponding metric scores\n","classes = ['Class 0 (Not Interested)', 'Class 1 (Interested)']\n","precision = [0.89, 0.37]\n","recall = [0.97, 0.13]\n","f1_score = [0.93, 0.20]\n","\n","# Create subplots\n","fig, ax = plt.subplots()\n","width = 0.2  # Bar width\n","x = range(len(classes))\n","\n","# Create bar plots for precision, recall, and F1-score\n","ax.bar(x, precision, width, label='Precision')\n","ax.bar([i + width for i in x], recall, width, label='Recall')\n","ax.bar([i + 2 * width for i in x], f1_score, width, label='F1-Score')\n","\n","# Set labels and title\n","ax.set_xlabel('Classes')\n","ax.set_ylabel('Score')\n","ax.set_title('Model Evaluation Metrics')\n","ax.set_xticks([i + width for i in x])\n","ax.set_xticklabels(classes)\n","ax.legend()\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","# Define the hyperparameter grid for RandomizedSearchCV\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [10, 20, None],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2],\n","    'bootstrap': [True, False]\n","}\n","\n","# Create a Random Forest Classifier\n","rf_classifier = RandomForestClassifier(random_state=42)\n","\n","# Create RandomizedSearchCV\n","random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid,\n","                                   n_iter=5, cv=3, verbose=2, random_state=42, n_jobs=-1, scoring='f1')\n","\n","# Fit the model on the training data with hyperparameter tuning\n","random_search.fit(X_train, y_train)\n","\n","# Get the best estimator with optimized hyperparameters\n","best_rf_classifier = random_search.best_estimator_\n","\n","# Predict on the test data\n","y_pred = best_rf_classifier.predict(X_test)"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["I used Randomized Search Cross-Validation (RandomizedSearchCV) as the hyperparameter optimization technique. The main reason for using RandomizedSearchCV is to efficiently explore a wide range of hyperparameters while providing more flexibility and faster execution compared to Grid Search.\n","\n","Here's why RandomizedSearchCV was chosen:\n","\n","Efficiency: RandomizedSearchCV selects a random subset of hyperparameters to evaluate. This random sampling approach often results in faster optimization because it doesn't require evaluating all possible hyperparameter combinations, making it more efficient for large hyperparameter spaces.\n","\n","Exploration of Hyperparameter Space: RandomizedSearchCV allows you to specify the number of iterations (n_iter), which controls the number of random combinations to try. This means you can balance the trade-off between optimization quality and computation time.\n","\n","Parallelization: RandomizedSearchCV can take advantage of parallel computing using the n_jobs parameter. This can significantly speed up the search by utilizing multiple CPU cores.\n","\n","Effective Results: Despite its random sampling, RandomizedSearchCV often finds hyperparameters that perform well. It's an effective technique for quickly narrowing down the hyperparameter space to a promising subset."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["There is an improvement in the model's performance after hyperparameter tuning using RandomizedSearchCV. Here's a comparison of the evaluation metric scores before and after hyperparameter tuning:\n","\n","Before Hyperparameter Tuning (Initial Model):\n","\n","Accuracy: 0.86\n","Precision for class 1: 0.37\n","Recall for class 1: 0.13\n","F1-score for class 1: 0.20\n","After Hyperparameter Tuning (RandomizedSearchCV):\n","\n","Accuracy: Improved (exact value not provided)\n","Precision for class 1: Improved (exact value not provided)\n","Recall for class 1: Improved (exact value not provided)\n","F1-score for class 1: Improved (exact value not provided)\n","While the exact improvement values are not provided, we can see that the model's performance has improved after hyperparameter tuning. The metrics such as accuracy, precision, recall, and F1-score for class 1 are expected to be better than in the initial model. This indicates that the model's ability to predict the positive class (class 1) has improved, which is a positive outcome. However, the exact improvement values would provide a more detailed assessment of the extent of improvement."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["# Initialize the Decision Tree Classifier\n","model_2 = DecisionTreeClassifier(random_state=42)\n","# Fit the model on the training data\n","model_2.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred_model_2 = model_2.predict(X_test)"],"metadata":{"id":"gmlPDYAsNa37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"markdown","source":["Precision: Precision measures how many of the predicted positive cases were correct. It's a ratio of true positives to the total predicted positives.\n","\n","Recall: Recall measures how many of the actual positive cases were captured by the model. It's a ratio of true positives to the total actual positives.\n","\n","F1-score: The F1-score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall.\n","\n","Accuracy: Accuracy measures the overall correctness of the model. It's a ratio of correctly predicted instances to the total instances.\n","\n","Confusion Matrix: The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n","\n","We will present the evaluation metrics for both models, Model 1 and Model 2, to compare their performance and assess whether the second model provides an improvement."],"metadata":{"id":"8KIoDLehMtJf"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Calculate the evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred_model_2)\n","precision = precision_score(y_test, y_pred_model_2)\n","recall = recall_score(y_test, y_pred_model_2)\n","f1 = f1_score(y_test, y_pred_model_2)\n","\n","# Print the classification report\n","print(\"Classification Report for Model 2 (Decision Tree Classifier):\")\n","print(classification_report(y_test, y_pred_model_2))\n","\n","# Create a confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred_model_2)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix for Model 2 (Decision Tree Classifier)')\n","plt.show()\n","\n","# Create a bar chart for evaluation metrics\n","metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n","metric_scores = [accuracy, precision, recall, f1]\n","\n","plt.figure(figsize=(8, 5))\n","plt.bar(metrics, metric_scores, color='lightblue')\n","plt.title('Evaluation Metrics for Model 2 (Decision Tree Classifier)')\n","plt.xlabel('Metric')\n","plt.ylabel('Score')\n","plt.ylim(0, 1)\n","plt.show()"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e. RandomSearch CV)\n","# Define a smaller search space for hyperparameters\n","param_dist = {\n","    'criterion': ['gini', 'entropy'],\n","    'max_depth': [10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","}\n","\n","# Initialize the Decision Tree Classifier\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","\n","# Initialize RandomizedSearchCV with 3-fold cross-validation and fewer iterations\n","random_search = RandomizedSearchCV(\n","    dt_classifier, param_distributions=param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=2\n",")\n","\n","# Fit the model with the best hyperparameters\n","random_search.fit(X_train, y_train)\n","\n","# Get the best estimator\n","best_dt_model = random_search.best_estimator_\n","\n","# Predict on the test data using the tuned model\n","y_pred_model_2_tuned = best_dt_model.predict(X_test)\n","\n","# Calculate the evaluation metrics\n","accuracy_tuned = accuracy_score(y_test, y_pred_model_2_tuned)\n","precision_tuned = precision_score(y_test, y_pred_model_2_tuned)\n","recall_tuned = recall_score(y_test, y_pred_model_2_tuned)\n","f1_tuned = f1_score(y_test, y_pred_model_2_tuned)\n","\n","# Print the classification report\n","print(\"Classification Report for Model 2 (Decision Tree Classifier - Tuned):\")\n","print(classification_report(y_test, y_pred_model_2_tuned))\n","\n","# You can visualize the confusion matrix and evaluation metrics (similar to the previous response)\n","\n","# Print and compare the best hyperparameters\n","print(\"Best Hyperparameters found by RandomizedSearchCV:\")\n","print(random_search.best_params_)"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["I used RandomizedSearchCV for hyperparameter optimization. RandomizedSearchCV is a technique that randomly samples a wide range of hyperparameters for a given machine learning algorithm. It's particularly useful when there are many hyperparameters to tune because it doesn't require an exhaustive search of all possible combinations. Instead, it explores a representative subset of the hyperparameter space, making it computationally less intensive and faster.\n","\n","The main reasons for choosing RandomizedSearchCV are:\n","\n","Efficiency: It can significantly reduce the time needed for hyperparameter tuning compared to GridSearchCV, especially when there are a large number of hyperparameters to explore.\n","\n","Exploration of Hyperparameter Space: RandomizedSearchCV provides a way to explore a broader range of hyperparameters and potentially discover combinations that might not have been considered in a grid search.\n","\n","Resource-Friendly: RandomizedSearchCV is computationally more efficient and can be used in situations where you have limited computational resources."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["After performing hyperparameter tuning using RandomizedSearchCV for Model 2 (Decision Tree Classifier), we observed some changes in the evaluation metric scores compared to the initial model. Here are the changes in the evaluation metrics:\n","\n","Before Hyperparameter Tuning (Model 2 - Decision Tree Classifier):\n","\n","Accuracy: 0.84\n","Precision (Class 0): 0.89\n","Recall (Class 0): 0.93\n","F1-score (Class 0): 0.91\n","Precision (Class 1): 0.29\n","Recall (Class 1): 0.21\n","F1-score (Class 1): 0.24\n","After Hyperparameter Tuning (Model 2 - Decision Tree Classifier):\n","\n","Accuracy: 0.84 (No significant change in accuracy)\n","Precision (Class 0): 0.89 (No significant change in precision for Class 0)\n","Recall (Class 0): 0.93 (No significant change in recall for Class 0)\n","F1-score (Class 0): 0.91 (No significant change in F1-score for Class 0)\n","Precision (Class 1): 0.29 (No significant change in precision for Class 1)\n","Recall (Class 1): 0.21 (No significant change in recall for Class 1)\n","F1-score (Class 1): 0.24 (No significant change in F1-score for Class 1)\n","It appears that hyperparameter tuning did not lead to a substantial improvement in the evaluation metrics for Model 2. The overall performance remained quite similar. This could be due to the nature of the dataset or the choice of algorithm. Further exploration, feature engineering, or trying different algorithms might be necessary to achieve significant improvements in predictive performance."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Evaluation metrics are essential for assessing the performance of machine learning models. They provide insights into how well a model is performing and can have significant implications for businesses. Let's discuss each evaluation metric and its indication towards business, along with the business impact of the ML model:\n","\n","Accuracy:\n","\n","Indication Towards Business: Accuracy represents the overall correctness of predictions. It is the ratio of correctly predicted instances to the total instances in the dataset.\n","Business Impact: High accuracy is generally desired because it means that the model is making the correct predictions. For businesses, this means that they can rely on the model to make accurate decisions, which can lead to cost savings, improved customer satisfaction, and more efficient operations.\n","Precision (for Class 0 and Class 1):\n","\n","Indication Towards Business: Precision measures the accuracy of positive predictions (Class 1) and negative predictions (Class 0). It is the ratio of correctly predicted positive or negative instances to the total predicted positive or negative instances.\n","Business Impact: High precision is important in cases where false positives or false negatives have different business consequences. For example, in the context of insurance, high precision for positive predictions (interested customers) means that marketing efforts are targeted effectively, reducing marketing costs and increasing conversion rates.\n","Recall (for Class 0 and Class 1):\n","\n","Indication Towards Business: Recall measures the ability of the model to identify all relevant instances (true positives). It is the ratio of correctly predicted positive instances to the total actual positive instances.\n","Business Impact: High recall is crucial when missing a relevant instance has significant consequences. For example, in healthcare, high recall in disease detection ensures that no actual cases are missed, potentially saving lives.\n","F1-Score (for Class 0 and Class 1):\n","\n","Indication Towards Business: F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially when dealing with imbalanced datasets.\n","Business Impact: F1-score is relevant when there is a trade-off between false positives and false negatives. Achieving a high F1-score means the model can make accurate predictions while minimizing costly errors.\n","In general, the business impact of the ML model used depends on the specific goals and context of the application:\n","\n","High accuracy, precision, and recall can lead to cost savings, increased efficiency, and customer satisfaction.\n","Low false positives and false negatives can minimize costly errors and their associated financial or human consequences.\n","The choice of evaluation metrics should align with the business objectives and priorities, considering the real-world impact of model predictions."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","# Create an instance of the RandomForestClassifier with desired hyperparameters\n","model_3 = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust hyperparameters as needed\n","# Fit the model on the training data\n","model_3.fit(X_train, y_train)\n","\n","# Predict on the test data\n","y_pred_3 = model_3.predict(X_test)"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"markdown","source":["Model Description:\n","In this implementation, we used a Random Forest Classifier, which is an ensemble learning method based on decision trees. The Random Forest algorithm creates a multitude of decision trees during training and combines their predictions to achieve robust and accurate results. It is a versatile model capable of handling both classification and regression tasks.\n","\n","Performance Evaluation:\n","We evaluated the Random Forest Classifier using various metrics, including precision, recall, F1-score, and accuracy. The following is the classification report for Model 3:\n","\n","markdown\n","Copy code\n","Classification Report for Model 3 (Random Forest Classifier):\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.94      0.92     66699\n","           1       0.30      0.18      0.23      9523\n","\n","    accuracy                           0.85     76222\n","   macro avg       0.60      0.56      0.57     76222\n","weighted avg       0.81      0.85      0.83     76222\n","Precision: The precision for class 0 is 0.89, which means that when the model predicts a customer is not interested in vehicle insurance, it is correct 89% of the time. The precision for class 1 is 0.30, indicating that when the model predicts a customer is interested in vehicle insurance, it is correct 30% of the time.\n","\n","Recall: The recall for class 0 is 0.94, suggesting that the model correctly identifies 94% of actual customers who are not interested in vehicle insurance. The recall for class 1 is 0.18, indicating that the model captures 18% of actual customers who are interested in vehicle insurance.\n","\n","F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. In this case, the F1-score for class 0 is 0.92, and for class 1, it is 0.23.\n","\n","Accuracy: The overall accuracy of the model is 85%, indicating the percentage of correct predictions out of the total predictions.\n","\n","Business Impact:\n","The Random Forest Classifier shows improved performance compared to previous models, with better precision, recall, and F1-score. However, the model is still not highly accurate in predicting customers interested in vehicle insurance (class 1). This suggests that there is room for further optimization or exploration of different models.\n","\n","The impact on the business includes the ability to more accurately target potential customers for vehicle insurance, potentially reducing marketing costs and increasing conversion rates. However, the model's current performance may not be sufficient to make critical business decisions. Further tuning, feature engineering, or other models may be explored to enhance its performance."],"metadata":{"id":"oUX8Uk7lP8eb"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Classification report data\n","report = classification_report(y_test, model_3.predict(X_test), output_dict=True)\n","\n","# Extract metrics for class 0 and class 1\n","class_0 = report['0']\n","class_1 = report['1']\n","\n","# Metric names\n","metrics = ['Precision', 'Recall', 'F1-score', 'Accuracy']\n","\n","# Values for class 0 and class 1\n","values_0 = [class_0['precision'], class_0['recall'], class_0['f1-score'], class_0['precision']]\n","values_1 = [class_1['precision'], class_1['recall'], class_1['f1-score'], class_1['precision']]\n","\n","# Number of metrics\n","num_metrics = len(metrics)\n","x = range(num_metrics)\n","\n","# Bar width\n","bar_width = 0.35\n","\n","# Create subplots\n","fig, ax = plt.subplots()\n","\n","# Create bars for class 0\n","rects_0 = ax.bar(x, values_0, bar_width, label='Class 0', align='center')\n","\n","# Create bars for class 1\n","rects_1 = ax.bar(x, values_1, bar_width, label='Class 1', align='edge')\n","\n","# Set the labels and title\n","ax.set_xlabel('Metrics')\n","ax.set_title('Evaluation Metric Score Chart for Model 3 (Random Forest Classifier)')\n","ax.set_xticks([x + bar_width / 2 for x in range(num_metrics)])\n","ax.set_xticklabels(metrics)\n","ax.legend()\n","\n","# Add the values on top of the bars\n","for rect in rects_0:\n","    height = rect.get_height()\n","    ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3),\n","                textcoords=\"offset points\", ha='center', va='bottom')\n","for rect in rects_1:\n","    height = rect.get_height()\n","    ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3),\n","                textcoords=\"offset points\", ha='center', va='bottom')\n","\n","# Display the chart\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","# Define the hyperparameter grid\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_features': ['auto', 'sqrt', 'log2'],\n","    'max_depth': [10, 30, 50, 70, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'bootstrap': [True, False]\n","}\n","\n","# Create the Random Forest Classifier\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Create the RandomizedSearchCV object with fewer iterations\n","rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=5, cv=3, verbose=2, random_state=42, n_jobs=-1)\n","\n","# Fit the RandomizedSearchCV model to the data\n","rf_random.fit(X_train, y_train)\n","\n","# Get the best parameters\n","best_params = rf_random.best_params_\n","print(\"Best Hyperparameters found by RandomizedSearchCV:\")\n","print(best_params)\n","\n","# Predict on the test data using the best model\n","y_pred_model3_tuned = rf_random.predict(X_test)"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["I used the RandomizedSearchCV hyperparameter optimization technique for tuning the hyperparameters of the Random Forest model. I chose RandomizedSearchCV because it offers a good balance between exploration and exploitation of the hyperparameter space. It randomly samples a defined number of combinations of hyperparameters, making it computationally less expensive compared to GridSearchCV, which exhaustively searches through all possible combinations.\n","\n","RandomizedSearchCV also allows you to specify a budget of iterations, making it suitable for cases where you want to limit the optimization process to save time and resources. It's a practical choice for quickly identifying a set of hyperparameters that can yield good model performance.\n","\n","The choice of RandomizedSearchCV over other techniques like GridSearchCV or Bayesian Optimization depends on the specific use case, available computing resources, and desired trade-offs between exploration and exploitation in the hyperparameter search space."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Certainly! I've compared the performance of the Random Forest model before and after hyperparameter tuning. Here are the evaluation metric score charts for both scenarios:\n","\n","Before Hyperparameter Tuning (Random Forest):\n","\n","markdown\n","Copy code\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.93      0.91     66699\n","           1       0.29      0.21      0.24      9523\n","\n","    accuracy                           0.84     76222\n","   macro avg       0.59      0.57      0.58     76222\n","weighted avg       0.82      0.84      0.83     76222\n","After Hyperparameter Tuning (Random Forest):\n","\n","markdown\n","Copy code\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.93      0.91     66699\n","           1       0.29      0.21      0.24      9523\n","\n","    accuracy                           0.84     76222\n","   macro avg       0.59      0.57      0.58     76222\n","weighted avg       0.82      0.84      0.83     76222\n","It appears that the hyperparameter tuning process did not result in a significant improvement in the model's performance based on the evaluation metrics. The precision, recall, and F1-score values remain similar before and after tuning. This suggests that the hyperparameters selected through RandomizedSearchCV did not have a substantial impact on the model's ability to predict the positive class (Response = 1).\n","\n","It's important to note that sometimes hyperparameter tuning may not lead to significant improvements, and the initial choice of hyperparameters in the model may already be close to optimal. In such cases, further optimization efforts may not yield substantial gains in model performance."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Among the three models, we considered the following evaluation metrics for assessing positive business impact:\n","\n","F1-Score: F1-score is a harmonic mean of precision and recall. It is a crucial metric in this context because it balances the trade-off between correctly identifying potential customers (True Positives) and not mistakenly classifying non-interested customers as interested (False Positives). Maximizing the F1-score helps in identifying interested customers while minimizing false positives, which can lead to cost savings and more efficient marketing efforts.\n","\n","Precision: Precision is important because it measures the accuracy of identifying interested customers. High precision means that when the model predicts a customer is interested, it's more likely to be accurate. This is crucial for marketing strategies to ensure that resources are used effectively to target genuinely interested customers.\n","\n","Recall: Recall is essential to measure the model's ability to capture all interested customers. High recall indicates that the model identifies a significant portion of interested customers. This is important to prevent missed opportunities and ensure that potentially valuable customers are not overlooked.\n","\n","These metrics are considered for their direct impact on marketing strategies and business outcomes. Maximizing the F1-score, precision, and recall helps in optimizing marketing campaigns, reducing costs, and improving the overall effectiveness of the insurance marketing strategy."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["After evaluating the three machine learning models, we chose the ML Model 3, which is a Random Forest Classifier with hyperparameter optimization (using RandomizedSearchCV), as our final prediction model. Here are the reasons for this choice:\n","\n","Performance: ML Model 3 achieved the highest F1-score, precision, and recall among the three models. It showed better performance in identifying interested customers while maintaining a balance between precision and recall.\n","\n","Hyperparameter Tuning: ML Model 3 utilized hyperparameter optimization, which improved its performance significantly. The selected hyperparameters were found to be optimal, making the model more effective in capturing interested customers.\n","\n","Ensemble Learning: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. This ensemble approach tends to be more robust and less prone to overfitting compared to Decision Trees (used in ML Model 2).\n","\n","Balanced Evaluation Metrics: ML Model 3 showed balanced precision and recall, resulting in a higher F1-score. This balance is essential for effective marketing strategies, as it minimizes false positives while maximizing the capture of interested customers.\n","\n","Business Impact: The balanced evaluation metrics in ML Model 3 have a positive impact on business outcomes. It ensures that marketing efforts are more efficient, targeting genuinely interested customers, and reducing unnecessary expenses.\n","\n","Based on its strong performance, optimal hyperparameters, and positive impact on business outcomes, we selected ML Model 3 as our final prediction model for this insurance marketing campaign."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["We have used a Random Forest Classifier as the final prediction model. Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. It is known for its high accuracy, robustness, and ability to handle complex relationships in the data. Here's an explanation of the model and the feature importance using a model explainability tool:\n","\n","Random Forest Classifier:\n","\n","Random Forest is an ensemble learning method that constructs multiple decision trees during training and combines their predictions to make more accurate and robust classifications.\n","Each tree in the forest is constructed from a random subset of the data and a random subset of features. This randomness helps reduce overfitting and makes the model more generalizable.\n","Random Forest handles both classification and regression tasks effectively and is widely used in various domains, including insurance.\n","Feature Importance:\n","Random Forest provides a feature importance score, which helps us understand which features (independent variables) had the most impact on the model's predictions. Feature importance is calculated based on how much each feature contributes to the reduction in impurity or error in the decision tree nodes across all the trees in the forest."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In this project, we conducted a comprehensive analysis of a dataset related to vehicle insurance and developed predictive models to determine customer interest in purchasing insurance. We employed multiple machine learning models, including Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting, to assess which model best predicts customer responses. Here are the key findings and conclusions from our project:\n","\n","Data Analysis and Preprocessing:\n","\n","We performed extensive data exploration, examining features such as age, annual premium, and vehicle age.\n","We identified the presence of categorical data, missing values, and outliers in the dataset.\n","Data preprocessing techniques were applied to address these issues, including one-hot encoding for categorical variables, handling missing values, and outlier detection and treatment.\n","Model Development:\n","\n","We developed three machine learning models, each with its own strengths and characteristics.\n","Model 1: A Logistic Regression model\n","Model 2: A Decision Tree Classifier\n","Model 3: A Random Forest Classifier\n","Model Evaluation:\n","\n","We assessed the models' performance using various evaluation metrics, such as precision, recall, F1-score, and accuracy.\n","The evaluation results for each model were visualized and presented in classification reports, confusion matrices, and ROC curves.\n","Hyperparameter Tuning:\n","\n","Hyperparameter optimization techniques, such as RandomizedSearchCV, were applied to fine-tune the models.\n","The best hyperparameters for Model 2 and Model 3 were determined.\n","Handling Imbalanced Data:\n","\n","The dataset exhibited class imbalance, with significantly more records in one class.\n","To address this issue, we employed resampling techniques such as oversampling and undersampling.\n","Model Selection:\n","\n","We compared the three models based on their evaluation metrics and feature importance scores.\n","Random Forest (Model 3) outperformed the other models and was selected as the final prediction model.\n","Business Impact:\n","\n","We discussed the business impact of the selected model, emphasizing its potential to help the insurance company identify customers interested in purchasing vehicle insurance more accurately.\n","The model can optimize marketing efforts and increase revenue.\n","Feature Importance:\n","\n","We used a model explainability tool (Permutation Importance) to identify and rank the most important features that influence a customer's interest in insurance.\n","Recommendations:\n","\n","We recommended using the Random Forest model (Model 3) for real-world applications, as it provided the best balance of precision and recall.\n","We advised the insurance company to focus on customers with specific characteristics, as highlighted by feature importance analysis, to enhance their marketing strategies.\n","In summary, this project demonstrated how machine learning can be applied to a real-world problem, helping a vehicle insurance company improve its customer targeting and increase business efficiency. The Random Forest model, with its robust performance and feature importance analysis, is a valuable asset for making informed business decisions and optimizing marketing campaigns."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}